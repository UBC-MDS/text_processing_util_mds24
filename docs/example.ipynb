{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example usage\n",
    "\n",
    "This package, `text_processing_util_mds24`, includes four functions for processing and representing text data for machine learning tasks, specifically natural language processing. It provides three different functions for text representations that take a list of documents in the form of raw text: `frequency_vectorizer`, `tfidf_vectorizer` and `tokenizer_padding`. If users wish to represent text in another way, `text_clean` will make their lives easier by converting all characters to lower case, removing all punctuations and numbers, and splitting each document into a list of words. Examples on how to use these functions are documented on this page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_processing_util_mds24 import text_clean, frequency_vectorizer, tfidf_vectorizer, tokenizer_padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Text Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first create a sample list of documents using the first paragraph of _On the Origin of Species_ by Charles Darwin. Here, each sentence in the paragraph is an individual document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_of_species = [\"When on board H.M.S. Beagle, as naturalist, I was much struck with certain facts in the distribution \" \\\n",
    "                     + \"of the organic beings inhabiting South America, and in the geological relations of the present to the \" \\\n",
    "                     + \"past inhabitants of that continent.\",\n",
    "                     \"These facts, as will be seen in the latter chapters of this volume, \" \\\n",
    "                     + \"seemed to throw some light on the origin of species—that mystery of mysteries, as it has been called by \" \\\n",
    "                     + \"one of our greatest philosophers.\",\n",
    "                     \"On my return home, it occurred to me, in 1837, that something might \" \\\n",
    "                     + \"perhaps be made out on this question by patiently accumulating and reflecting on all sorts of facts which \" \\\n",
    "                     + \"could possibly have any bearing on it.\",\n",
    "                     \"After five years’ work I allowed myself to speculate on the subject, \" \\\n",
    "                     + \"and drew up some short notes; these I enlarged in 1844 into a sketch of the conclusions, which then seemed \" \\\n",
    "                     + \"to me probable: from that period to the present day I have steadily pursued the same object.\",\n",
    "                     \"I hope that I may be excused for entering on these personal details, as I give them to show that \" \\\n",
    "                     + \"I have not been hasty in coming to a decision.\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`text_clean()` cleans raw text for further text processing. This function will convert all characters to lower case, remove punctuations as well as numbers, and split words by spaces. All other functions in this package will call `text_clean()` before transforming the text to other representations, and therefore accept raw text as input. The user can also use this function to clean texts before feeding the texts to another algorithm of their choice.\n",
    "The usage of this function is demonstrated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['when', 'on', 'board', 'hms', 'beagle', 'as', 'naturalist', 'i', 'was', 'much', 'struck', 'with', 'certain', 'facts', 'in', 'the', 'distribution', 'of', 'the', 'organic', 'beings', 'inhabiting', 'south', 'america', 'and', 'in', 'the', 'geological', 'relations', 'of', 'the', 'present', 'to', 'the', 'past', 'inhabitants', 'of', 'that', 'continent']\n",
      "\n",
      "['these', 'facts', 'as', 'will', 'be', 'seen', 'in', 'the', 'latter', 'chapters', 'of', 'this', 'volume', 'seemed', 'to', 'throw', 'some', 'light', 'on', 'the', 'origin', 'of', 'species—that', 'mystery', 'of', 'mysteries', 'as', 'it', 'has', 'been', 'called', 'by', 'one', 'of', 'our', 'greatest', 'philosophers']\n",
      "\n",
      "['on', 'my', 'return', 'home', 'it', 'occurred', 'to', 'me', 'in', 'that', 'something', 'might', 'perhaps', 'be', 'made', 'out', 'on', 'this', 'question', 'by', 'patiently', 'accumulating', 'and', 'reflecting', 'on', 'all', 'sorts', 'of', 'facts', 'which', 'could', 'possibly', 'have', 'any', 'bearing', 'on', 'it']\n",
      "\n",
      "['after', 'five', 'years’', 'work', 'i', 'allowed', 'myself', 'to', 'speculate', 'on', 'the', 'subject', 'and', 'drew', 'up', 'some', 'short', 'notes', 'these', 'i', 'enlarged', 'in', 'into', 'a', 'sketch', 'of', 'the', 'conclusions', 'which', 'then', 'seemed', 'to', 'me', 'probable', 'from', 'that', 'period', 'to', 'the', 'present', 'day', 'i', 'have', 'steadily', 'pursued', 'the', 'same', 'object']\n",
      "\n",
      "['i', 'hope', 'that', 'i', 'may', 'be', 'excused', 'for', 'entering', 'on', 'these', 'personal', 'details', 'as', 'i', 'give', 'them', 'to', 'show', 'that', 'i', 'have', 'not', 'been', 'hasty', 'in', 'coming', 'to', 'a', 'decision']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleaned_txt = text_clean(origin_of_species)\n",
    "for c in cleaned_txt:\n",
    "    print(str(c) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to cleaning the text, the package provides three different text representations to be used for machine learning models: frequency vectorizer, TF-IDF vectorizer and tokenizer plus padding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Representation 1: Frequency Vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The frequency_vectorizer calculates the frequency of each word in a list of text documents. This function is useful for transforming text data into a feature matrix, and word frequency matrix.\n",
    "\n",
    "The usage of this function is demonstrated below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency Matrix:\n",
      "[[0.         0.         0.         0.         0.         0.02564103\n",
      "  0.02564103 0.         0.02564103 0.         0.02564103 0.\n",
      "  0.         0.02564103 0.02564103 0.         0.         0.02564103\n",
      "  0.         0.         0.         0.02564103 0.         0.\n",
      "  0.         0.         0.02564103 0.         0.         0.\n",
      "  0.         0.02564103 0.         0.         0.         0.02564103\n",
      "  0.         0.         0.         0.         0.         0.02564103\n",
      "  0.         0.         0.02564103 0.05128205 0.02564103 0.02564103\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.02564103 0.         0.         0.\n",
      "  0.         0.02564103 0.         0.         0.         0.\n",
      "  0.07692308 0.02564103 0.         0.02564103 0.         0.\n",
      "  0.         0.02564103 0.         0.         0.         0.\n",
      "  0.         0.         0.02564103 0.         0.         0.\n",
      "  0.         0.02564103 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.02564103 0.         0.         0.         0.02564103 0.\n",
      "  0.02564103 0.12820513 0.         0.         0.         0.\n",
      "  0.         0.02564103 0.         0.         0.02564103 0.02564103\n",
      "  0.         0.         0.02564103 0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.05405405 0.02702703 0.         0.\n",
      "  0.02702703 0.         0.         0.02702703 0.02702703 0.\n",
      "  0.02702703 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.02702703 0.         0.         0.         0.\n",
      "  0.         0.02702703 0.02702703 0.         0.         0.\n",
      "  0.         0.         0.         0.02702703 0.         0.\n",
      "  0.         0.02702703 0.02702703 0.02702703 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.02702703\n",
      "  0.02702703 0.         0.         0.         0.         0.\n",
      "  0.10810811 0.02702703 0.02702703 0.         0.02702703 0.02702703\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.02702703 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.02702703 0.02702703\n",
      "  0.         0.         0.         0.02702703 0.         0.\n",
      "  0.         0.02702703 0.         0.         0.         0.\n",
      "  0.         0.05405405 0.         0.         0.02702703 0.02702703\n",
      "  0.02702703 0.02702703 0.         0.02702703 0.         0.\n",
      "  0.         0.02702703 0.         0.         0.        ]\n",
      " [0.         0.02702703 0.         0.02702703 0.         0.\n",
      "  0.02702703 0.02702703 0.         0.02702703 0.         0.02702703\n",
      "  0.         0.         0.         0.02702703 0.         0.\n",
      "  0.         0.         0.         0.         0.02702703 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.02702703 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.02702703 0.\n",
      "  0.02702703 0.         0.         0.02702703 0.         0.\n",
      "  0.         0.05405405 0.         0.         0.02702703 0.\n",
      "  0.02702703 0.02702703 0.         0.02702703 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.02702703\n",
      "  0.02702703 0.10810811 0.         0.         0.         0.\n",
      "  0.02702703 0.         0.02702703 0.02702703 0.         0.\n",
      "  0.         0.02702703 0.         0.         0.         0.02702703\n",
      "  0.02702703 0.         0.02702703 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.02702703 0.02702703\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.02702703 0.         0.         0.         0.         0.02702703\n",
      "  0.         0.02702703 0.         0.         0.         0.\n",
      "  0.02702703 0.         0.         0.         0.        ]\n",
      " [0.02083333 0.         0.02083333 0.         0.02083333 0.\n",
      "  0.02083333 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.02083333 0.         0.         0.02083333\n",
      "  0.         0.         0.         0.02083333 0.02083333 0.\n",
      "  0.         0.         0.02083333 0.         0.02083333 0.\n",
      "  0.         0.         0.         0.         0.02083333 0.\n",
      "  0.         0.         0.0625     0.02083333 0.         0.\n",
      "  0.02083333 0.         0.         0.         0.         0.\n",
      "  0.02083333 0.         0.         0.         0.02083333 0.\n",
      "  0.         0.         0.         0.02083333 0.02083333 0.\n",
      "  0.02083333 0.02083333 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.02083333 0.\n",
      "  0.         0.         0.02083333 0.02083333 0.02083333 0.\n",
      "  0.         0.         0.         0.02083333 0.02083333 0.\n",
      "  0.02083333 0.         0.02083333 0.02083333 0.         0.\n",
      "  0.         0.         0.02083333 0.02083333 0.         0.02083333\n",
      "  0.02083333 0.08333333 0.         0.02083333 0.02083333 0.\n",
      "  0.         0.0625     0.02083333 0.         0.         0.\n",
      "  0.02083333 0.         0.         0.02083333 0.02083333]\n",
      " [0.03333333 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.03333333 0.03333333 0.         0.\n",
      "  0.03333333 0.         0.         0.         0.         0.\n",
      "  0.         0.03333333 0.         0.         0.         0.\n",
      "  0.03333333 0.03333333 0.         0.         0.         0.03333333\n",
      "  0.03333333 0.         0.         0.03333333 0.         0.\n",
      "  0.03333333 0.         0.         0.03333333 0.03333333 0.\n",
      "  0.         0.03333333 0.13333333 0.03333333 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.03333333\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.03333333 0.         0.         0.\n",
      "  0.         0.03333333 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.03333333\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.03333333 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.06666667 0.         0.03333333 0.         0.03333333 0.\n",
      "  0.         0.06666667 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]]\n",
      "\n",
      "Feature Names:\n",
      "['a', 'accumulating', 'after', 'all', 'allowed', 'america', 'and', 'any', 'as', 'be', 'beagle', 'bearing', 'been', 'beings', 'board', 'by', 'called', 'certain', 'chapters', 'coming', 'conclusions', 'continent', 'could', 'day', 'decision', 'details', 'distribution', 'drew', 'enlarged', 'entering', 'excused', 'facts', 'five', 'for', 'from', 'geological', 'give', 'greatest', 'has', 'hasty', 'have', 'hms', 'home', 'hope', 'i', 'in', 'inhabitants', 'inhabiting', 'into', 'it', 'latter', 'light', 'made', 'may', 'me', 'might', 'much', 'my', 'myself', 'mysteries', 'mystery', 'naturalist', 'not', 'notes', 'object', 'occurred', 'of', 'on', 'one', 'organic', 'origin', 'our', 'out', 'past', 'patiently', 'perhaps', 'period', 'personal', 'philosophers', 'possibly', 'present', 'probable', 'pursued', 'question', 'reflecting', 'relations', 'return', 'same', 'seemed', 'seen', 'short', 'show', 'sketch', 'some', 'something', 'sorts', 'south', 'species—that', 'speculate', 'steadily', 'struck', 'subject', 'that', 'the', 'them', 'then', 'these', 'this', 'throw', 'to', 'up', 'volume', 'was', 'when', 'which', 'will', 'with', 'work', 'years’']\n"
     ]
    }
   ],
   "source": [
    "tf_matrix, tf_feature_names = frequency_vectorizer(origin_of_species)\n",
    "\n",
    "print(\"Frequency Matrix:\")\n",
    "print(tf_matrix)\n",
    "print(\"\\nFeature Names:\")\n",
    "print(tf_feature_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Representation 2: TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `tfidf_vectorizer` function computes the Term Frequency-Inverse Document Frequency (TF-IDF) scores for a given list of documents, providing a numerical representation that highlights the importance of terms within the context of the entire document set. This function is useful for transforming text data into a feature matrix, capturing the significance of terms while considering their frequency and uniqueness across the document collection.\n",
    "\n",
    "The usage of this function is demonstrated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vecotrized documents:\n",
      "[ 0.          0.          0.          0.          0.          0.02349463\n",
      "  0.00572163  0.          0.00572163  0.          0.02349463  0.\n",
      "  0.          0.02349463  0.02349463  0.          0.          0.02349463\n",
      "  0.          0.          0.          0.02349463  0.          0.\n",
      "  0.          0.          0.02349463  0.          0.          0.\n",
      "  0.          0.00572163  0.          0.          0.          0.02349463\n",
      "  0.          0.          0.          0.          0.          0.02349463\n",
      "  0.          0.          0.00572163 -0.00934982  0.02349463  0.02349463\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.02349463  0.          0.          0.\n",
      "  0.          0.02349463  0.          0.          0.          0.\n",
      "  0.         -0.00467491  0.          0.02349463  0.          0.\n",
      "  0.          0.02349463  0.          0.          0.          0.\n",
      "  0.          0.          0.01309809  0.          0.          0.\n",
      "  0.          0.02349463  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.02349463  0.          0.          0.          0.02349463  0.\n",
      "  0.          0.02860815  0.          0.          0.          0.\n",
      "  0.         -0.00467491  0.          0.          0.02349463  0.02349463\n",
      "  0.          0.          0.02349463  0.          0.        ]\n",
      "[ 0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.01206181  0.00603091  0.          0.\n",
      "  0.0138061   0.          0.          0.0138061   0.02476461  0.\n",
      "  0.02476461  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.00603091  0.          0.          0.          0.\n",
      "  0.          0.02476461  0.02476461  0.          0.          0.\n",
      "  0.          0.          0.         -0.00492761  0.          0.\n",
      "  0.          0.0138061   0.02476461  0.02476461  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.02476461\n",
      "  0.02476461  0.          0.          0.          0.          0.\n",
      "  0.         -0.00492761  0.02476461  0.          0.02476461  0.02476461\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.02476461  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.0138061   0.02476461\n",
      "  0.          0.          0.          0.0138061   0.          0.\n",
      "  0.          0.02476461  0.          0.          0.          0.\n",
      "  0.          0.01206181  0.          0.          0.00603091  0.0138061\n",
      "  0.02476461 -0.00492761  0.          0.02476461  0.          0.\n",
      "  0.          0.02476461  0.          0.          0.        ]\n",
      "[ 0.          0.02476461  0.          0.02476461  0.          0.\n",
      "  0.00603091  0.02476461  0.          0.00603091  0.          0.02476461\n",
      "  0.          0.          0.          0.0138061   0.          0.\n",
      "  0.          0.          0.          0.          0.02476461  0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.00603091  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.00603091  0.\n",
      "  0.02476461  0.          0.         -0.00492761  0.          0.\n",
      "  0.          0.0276122   0.          0.          0.02476461  0.\n",
      "  0.0138061   0.02476461  0.          0.02476461  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.02476461\n",
      "  0.         -0.01971044  0.          0.          0.          0.\n",
      "  0.02476461  0.          0.02476461  0.02476461  0.          0.\n",
      "  0.          0.02476461  0.          0.          0.          0.02476461\n",
      "  0.02476461  0.          0.02476461  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.02476461  0.02476461\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.0138061\n",
      "  0.         -0.00492761  0.          0.          0.          0.\n",
      "  0.0138061   0.          0.          0.          0.        ]\n",
      "[ 0.0106422   0.          0.01908939  0.          0.01908939  0.\n",
      "  0.00464882  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.01908939  0.          0.          0.01908939\n",
      "  0.          0.          0.          0.01908939  0.01908939  0.\n",
      "  0.          0.          0.01908939  0.          0.01908939  0.\n",
      "  0.          0.          0.          0.          0.00464882  0.\n",
      "  0.          0.          0.01394647 -0.00379837  0.          0.\n",
      "  0.01908939  0.          0.          0.          0.          0.\n",
      "  0.0106422   0.          0.          0.          0.01908939  0.\n",
      "  0.          0.          0.          0.01908939  0.01908939  0.\n",
      "  0.         -0.00379837  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.01908939  0.\n",
      "  0.          0.          0.0106422   0.01908939  0.01908939  0.\n",
      "  0.          0.          0.          0.01908939  0.0106422   0.\n",
      "  0.01908939  0.          0.01908939  0.0106422   0.          0.\n",
      "  0.          0.          0.01908939  0.01908939  0.          0.01908939\n",
      "  0.          0.0185953   0.          0.01908939  0.00464882  0.\n",
      "  0.         -0.0113951   0.01908939  0.          0.          0.\n",
      "  0.0106422   0.          0.          0.01908939  0.01908939]\n",
      "[ 0.01702752  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.00743812  0.00743812  0.          0.\n",
      "  0.01702752  0.          0.          0.          0.          0.\n",
      "  0.          0.03054302  0.          0.          0.          0.\n",
      "  0.03054302  0.03054302  0.          0.          0.          0.03054302\n",
      "  0.03054302  0.          0.          0.03054302  0.          0.\n",
      "  0.03054302  0.          0.          0.03054302  0.00743812  0.\n",
      "  0.          0.03054302  0.02975247 -0.00607739  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.03054302\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.03054302  0.          0.          0.\n",
      "  0.         -0.00607739  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.03054302\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.03054302  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.03054302  0.          0.00743812  0.\n",
      "  0.         -0.01215477  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.        ]\n",
      "\n",
      "Feature names:\n",
      "['a', 'accumulating', 'after', 'all', 'allowed', 'america', 'and', 'any', 'as', 'be', 'beagle', 'bearing', 'been', 'beings', 'board', 'by', 'called', 'certain', 'chapters', 'coming', 'conclusions', 'continent', 'could', 'day', 'decision', 'details', 'distribution', 'drew', 'enlarged', 'entering', 'excused', 'facts', 'five', 'for', 'from', 'geological', 'give', 'greatest', 'has', 'hasty', 'have', 'hms', 'home', 'hope', 'i', 'in', 'inhabitants', 'inhabiting', 'into', 'it', 'latter', 'light', 'made', 'may', 'me', 'might', 'much', 'my', 'myself', 'mysteries', 'mystery', 'naturalist', 'not', 'notes', 'object', 'occurred', 'of', 'on', 'one', 'organic', 'origin', 'our', 'out', 'past', 'patiently', 'perhaps', 'period', 'personal', 'philosophers', 'possibly', 'present', 'probable', 'pursued', 'question', 'reflecting', 'relations', 'return', 'same', 'seemed', 'seen', 'short', 'show', 'sketch', 'some', 'something', 'sorts', 'south', 'species—that', 'speculate', 'steadily', 'struck', 'subject', 'that', 'the', 'them', 'then', 'these', 'this', 'throw', 'to', 'up', 'volume', 'was', 'when', 'which', 'will', 'with', 'work', 'years’']\n"
     ]
    }
   ],
   "source": [
    "text_tdidf_vectorized, feature_names = tfidf_vectorizer(origin_of_species)\n",
    "\n",
    "print(\"Vecotrized documents:\")\n",
    "for vectorized_doc in text_tdidf_vectorized:\n",
    "    print(vectorized_doc)\n",
    "\n",
    "print(\"\\nFeature names:\")\n",
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Representation 3: Tokenizer and Padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you would like to feed the data to recurrent neural networks (RNNs), you can use transform your text with `tokenizer_padding`. This function converts each word into an individual token represented by a number, but keeps the order of the original sentence, which is important for RNNs. It also pads shorter sequences with zeros at the end because deep learning libraries generally do not accept sequences of uneven lengths.\n",
    "\n",
    "The usage of this function is demonstrated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.,  11.,\n",
       "         12.,  13.,  14.,  15.,  16.,  17.,  18.,  16.,  19.,  20.,  21.,\n",
       "         22.,  23.,  24.,  15.,  16.,  25.,  26.,  18.,  16.,  27.,  28.,\n",
       "         16.,  29.,  30.,  18.,  31.,  32.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.],\n",
       "       [ 33.,  14.,   6.,  34.,  35.,  36.,  15.,  16.,  37.,  38.,  18.,\n",
       "         39.,  40.,  41.,  28.,  42.,  43.,  44.,   2.,  16.,  45.,  18.,\n",
       "         46.,  47.,  18.,  48.,   6.,  49.,  50.,  51.,  52.,  53.,  54.,\n",
       "         18.,  55.,  56.,  57.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.],\n",
       "       [  2.,  58.,  59.,  60.,  49.,  61.,  28.,  62.,  15.,  31.,  63.,\n",
       "         64.,  65.,  35.,  66.,  67.,   2.,  39.,  68.,  53.,  69.,  70.,\n",
       "         24.,  71.,   2.,  72.,  73.,  18.,  14.,  74.,  75.,  76.,  77.,\n",
       "         78.,  79.,   2.,  49.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.],\n",
       "       [ 80.,  81.,  82.,  83.,   8.,  84.,  85.,  28.,  86.,   2.,  16.,\n",
       "         87.,  24.,  88.,  89.,  43.,  90.,  91.,  33.,   8.,  92.,  15.,\n",
       "         93.,  94.,  95.,  18.,  16.,  96.,  74.,  97.,  41.,  28.,  62.,\n",
       "         98.,  99.,  31., 100.,  28.,  16.,  27., 101.,   8.,  77., 102.,\n",
       "        103.,  16., 104., 105.],\n",
       "       [  8., 106.,  31.,   8., 107.,  35., 108., 109., 110.,   2.,  33.,\n",
       "        111., 112.,   6.,   8., 113., 114.,  28., 115.,  31.,   8.,  77.,\n",
       "        116.,  51., 117.,  15., 118.,  28.,  94., 119.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_tokenized_padded = tokenizer_padding(origin_of_species)\n",
    "text_tokenized_padded"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:text_processing]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
